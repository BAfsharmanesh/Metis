{
  "model": {
    "model_name": "llama2_1B",
    "num_layers": 18,
    "parameters": {
      "total_parameters_bytes": 4223967232.0,
      "parameters_per_layer_bytes": [
        262144000,
        205537280.0,
        205537280.0,
        205537280.0,
        205537280.0,
        205537280.0,
        205537280.0,
        205537280.0,
        205537280.0,
        205537280.0,
        205537280.0,
        205537280.0,
        205537280.0,
        205537280.0,
        205537280.0,
        205537280.0,
        205537280.0,
        205537280.0,
        205537280.0,
        8192,
        262144000
      ],
      "activation_parameters_bytes": [
        536870912,
        8321499136.0,
        8321499136.0,
        8321499136.0,
        8321499136.0,
        8321499136.0,
        8321499136.0,
        8321499136.0,
        8321499136.0,
        8321499136.0,
        8321499136.0,
        8321499136.0,
        8321499136.0,
        8321499136.0,
        8321499136.0,
        8321499136.0,
        8321499136.0,
        8321499136.0,
        8321499136.0,
        536870912,
        8388608000
      ]
    }
  },
  "execution_time": {
    "total_time_ms": 13983.415650719762,
    "forward_backward_time_ms": 13715.367708887374,
    "batch_generator_time_ms": 161.20968546186174,
    "layernorm_grads_all_reduce_time_ms": null,
    "embedding_grads_all_reduce_time_ms": null,
    "optimizer_time_ms": 83.31226901788425,
    "layer_compute_total_ms": [
      20.2333927154541,
      719.2579252379281,
      719.2579252379281,
      719.2579252379281,
      719.2579252379281,
      719.2579252379281,
      719.2579252379281,
      719.2579252379281,
      719.2579252379281,
      719.2579252379281,
      719.2579252379281,
      719.2579252379281,
      719.2579252379281,
      719.2579252379281,
      719.2579252379281,
      719.2579252379281,
      719.2579252379281,
      719.2579252379281,
      719.2579252379281,
      12.042828968593051,
      579.2323861803327
    ]
  },
  "execution_memory": {
    "total_memory_mb": 268991.7513885498,
    "layer_memory_total_mb": [
      1251.0000076293945,
      12565.125099182129,
      12565.125099182129,
      12565.125099182129,
      12565.125099182129,
      12565.125099182129,
      12565.125099182129,
      12565.125099182129,
      12565.125099182129,
      12565.125099182129,
      12565.125099182129,
      12565.125099182129,
      12565.125099182129,
      12565.125099182129,
      12565.125099182129,
      12565.125099182129,
      12565.125099182129,
      12565.125099182129,
      12565.125099182129,
      1024.3125076293945,
      2024.0000076293945
    ]
  }
}