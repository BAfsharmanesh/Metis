{
  "model": {
    "model_name": "llama2_1B",
    "num_layers": 18,
    "parameters": {
      "total_parameters_bytes": 4223967232.0,
      "parameters_per_layer_bytes": [
        262144000,
        205537280.0,
        205537280.0,
        205537280.0,
        205537280.0,
        205537280.0,
        205537280.0,
        205537280.0,
        205537280.0,
        205537280.0,
        205537280.0,
        205537280.0,
        205537280.0,
        205537280.0,
        205537280.0,
        205537280.0,
        205537280.0,
        205537280.0,
        205537280.0,
        8192,
        262144000
      ],
      "activation_parameters_bytes": [
        33554432,
        520093696.0,
        520093696.0,
        520093696.0,
        520093696.0,
        520093696.0,
        520093696.0,
        520093696.0,
        520093696.0,
        520093696.0,
        520093696.0,
        520093696.0,
        520093696.0,
        520093696.0,
        520093696.0,
        520093696.0,
        520093696.0,
        520093696.0,
        520093696.0,
        33554432,
        524288000
      ]
    }
  },
  "execution_time": {
    "total_time_ms": 751.9174763540655,
    "forward_backward_time_ms": 658.7365014212472,
    "batch_generator_time_ms": 11.40352657863072,
    "layernorm_grads_all_reduce_time_ms": null,
    "embedding_grads_all_reduce_time_ms": null,
    "optimizer_time_ms": 55.26363342804002,
    "layer_compute_total_ms": [
      4.321200507027763,
      34.209745270865305,
      34.209745270865305,
      34.209745270865305,
      34.209745270865305,
      34.209745270865305,
      34.209745270865305,
      34.209745270865305,
      34.209745270865305,
      34.209745270865305,
      34.209745270865305,
      34.209745270865305,
      34.209745270865305,
      34.209745270865305,
      34.209745270865305,
      34.209745270865305,
      34.209745270865305,
      34.209745270865305,
      34.209745270865305,
      3.0707631792340964,
      16.780853271484375
    ]
  },
  "execution_memory": {
    "total_memory_mb": 33445.44027709961,
    "layer_memory_total_mb": [
      1750.125015258789,
      1649.5314483642578,
      1649.5314483642578,
      1649.5314483642578,
      1649.5314483642578,
      1649.5314483642578,
      1649.5314483642578,
      1649.5314483642578,
      1649.5314483642578,
      1649.5314483642578,
      1649.5314483642578,
      1649.5314483642578,
      1649.5314483642578,
      1649.5314483642578,
      1649.5314483642578,
      1649.5314483642578,
      1649.5314483642578,
      1649.5314483642578,
      1649.5314483642578,
      64.14064025878906,
      1128.000015258789
    ]
  }
}