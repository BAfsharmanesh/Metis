{
  "model": {
    "model_name": "llama2_1B",
    "num_layers": 18,
    "parameters": {
      "total_parameters_bytes": 4223967232.0,
      "parameters_per_layer_bytes": [
        262144000,
        205537280.0,
        205537280.0,
        205537280.0,
        205537280.0,
        205537280.0,
        205537280.0,
        205537280.0,
        205537280.0,
        205537280.0,
        205537280.0,
        205537280.0,
        205537280.0,
        205537280.0,
        205537280.0,
        205537280.0,
        205537280.0,
        205537280.0,
        205537280.0,
        8192,
        262144000
      ],
      "activation_parameters_bytes": [
        16777216,
        260046848.0,
        260046848.0,
        260046848.0,
        260046848.0,
        260046848.0,
        260046848.0,
        260046848.0,
        260046848.0,
        260046848.0,
        260046848.0,
        260046848.0,
        260046848.0,
        260046848.0,
        260046848.0,
        260046848.0,
        260046848.0,
        260046848.0,
        260046848.0,
        16777216,
        262144000
      ]
    }
  },
  "execution_time": {
    "total_time_ms": 275.30724831138826,
    "forward_backward_time_ms": 238.10465233742062,
    "batch_generator_time_ms": 2.9994385780530215,
    "layernorm_grads_all_reduce_time_ms": null,
    "embedding_grads_all_reduce_time_ms": null,
    "optimizer_time_ms": 21.954748890351787,
    "layer_compute_total_ms": [
      1.4655927724873241,
      12.352207045329012,
      12.352207045329012,
      12.352207045329012,
      12.352207045329012,
      12.352207045329012,
      12.352207045329012,
      12.352207045329012,
      12.352207045329012,
      12.352207045329012,
      12.352207045329012,
      12.352207045329012,
      12.352207045329012,
      12.352207045329012,
      12.352207045329012,
      12.352207045329012,
      12.352207045329012,
      12.352207045329012,
      12.352207045329012,
      1.3781106540703538,
      3.0222210348442133
    ]
  },
  "execution_memory": {
    "total_memory_mb": 3692.6217880249023,
    "layer_memory_total_mb": [
      343.76562881469727,
      168.33012771606445,
      168.33012771606445,
      168.33012771606445,
      168.33012771606445,
      168.33012771606445,
      168.33012771606445,
      168.33012771606445,
      168.33012771606445,
      168.33012771606445,
      168.33012771606445,
      168.33012771606445,
      168.33012771606445,
      168.33012771606445,
      168.33012771606445,
      168.33012771606445,
      168.33012771606445,
      168.33012771606445,
      168.33012771606445,
      4.032230377197266,
      141.00000381469727
    ]
  }
}